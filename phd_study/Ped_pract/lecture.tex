\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[margin=1.5cm]{geometry}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\newtheorem{definition}{Определение}

\newcommand{\M}{{\mathsf M}\,}
\newcommand{\cov}{\mathsf{cov}}
\newcommand{\corr}{\mathsf{corr}}
\newcommand{\var}{\mathsf{D}\,}
\renewcommand{\Pr}{{\mathsf P}}
\newcommand*{\hm}[1]{#1\nobreak\discretionary{}%
  {\hbox{$\mathsurround=0pt #1$}}{}} 

\begin{document}
\begin{center}
  \Large
  Лекция 19/05/2017
\end{center}

\bigskip

На прошлых двух лекциях речь шла о числовых характеристиках одной случайной величины
(мат.ожидание, дисперия, мода, медиана). Сегодня мы поговорим о числовых
характеристиках \emph{совокупностей} слуайных величин.

Пусть задано совместное распределение случайных величин $X_1$, $X_2$, \ldots $X_n$ (в
дискретном варианте --- с помощью совместного распределения вероятностей 
\[
p(a, b, \ldots, c) = \Pr(X_1=a, X_2=b, \ldots, X_n=c),
\]
а в непрерывном случае --- с помощью совместной плотности распределения $p(u, v, \ldots,
w)$. Пусть далее $y=h(x_1, x_2, \ldots, x_n)$ --- <<хорошая>> функция (это требование, как
обычно, означает, что результат подстановки случайных величин $X_1$, $X_2$, \ldots $X_n$ в
качестве аргументов дает снова случайную величину $Y=h(X_1, X_2, \ldots,
X_n)$)\footnote{Поскольку это вечерники и часов у них мало, мы не <<ругаемся>> словами
  типа <<измеримая функция>>, на это не хватает ни времени, ни общего уровня подготовки}. 
Тогда математическое ожидание величины $Y$ может быть найдено по формулам:
\[
\M h(X_1, X_2, \ldots, X_n) = 
\begin{cases}
  \displaystyle\sum\limits_{a, b, \ldots, c} h(a, b, \ldots, c) p(a, b, \ldots, c) & \text{для дискретных
    величин,}\\
  \displaystyle\idotsint\limits_{R^n} h(u, v, \ldots, w) p(u, v, \ldots, w)\,dudv\cdots dw & \text{для
    непрерывного вектора.}
\end{cases}
\]
\begin{definition}
  Пусть $X$, $Y$ --- случайные величины с конечными математическими ожиданиями $\M X$ и
  $\M Y$. Число 
  \[
  \cov(X,Y)=\M(X-\M X)(Y-\M Y)
  \]
  называется \emph{ковариацией} между случайными величинами $X$ и $Y$.
\end{definition}

Здесь используется функция $h(u,v)=(u-\M X)(v-\M Y)$ и вычисление следует проводить на
основании формулы выше.

\underline{Пример.} Рассмотрим двумерное нормальное распределение (опять) с плотностью
\[
p(u,v)=\dfrac{1}{2\pi\sigma_1\sigma_2\sqrt{1-r^2}}%
\exp\Bigl\{-\dfrac{1}{2(1-r^2)}\Bigl(
\Bigl(\dfrac{u-a}{\sigma_1}\Bigr)^{\!\!2}-2r\dfrac{(u-a)(v-b)}{\sigma_1\sigma_2}
+ \Bigl(\dfrac{v-b}{\sigma_2}\Bigr)^{\!\!2}\,\Bigr)\Bigr\}.
\]
На предыдущих лекции вам было доказано, что $X$ в этом случае имеет марнигальное
нормальное распределение с математическим ожиданием $a$ и дисперсией $\sigma_1^2$, а $Y$
имеет марнигальное нормальное распределение с математическим ожиданием $b$ и дисперсией
$\sigma_2^2$. Поэтому ковариация равна:
\[
  \cov(X,Y)=\iint_{R^2} (u-a)(v-b) p(u,v)\,dudv=
\]
{замена: $(u-a)/\sigma_1=x$, $(v-b)/\sigma_2=y$}
\[ 
=\sigma_1\sigma_2\iint_{R^2}\dfrac{xy}{2\pi\sqrt{1-r^2}}
\exp\Bigl\{-\dfrac{1}{2(1-r^2)}\Bigl( x^2-2rxy+y^2\Bigr)\Bigr\}\,dxdy
\]
Выделяем полный квадрат по $x$:
\[
 x^2-2rxy+y^2 = (x-ry)^2+(1-r^2)y^2,
\]
поэтому двойной интгерал равен повторному
\[
  \int\limits_{-\infty}^\infty \biggl(\dfrac{y}{\sqrt{2\pi}} e^{-{y^2}/{2}} \int\limits_{-\infty}^\infty 
  \dfrac{x}{\sqrt{2\pi(1-r^2)}} e^{-(x-ry)^2/(2(1-r^2))}\,dx\biggr)\,dy.
\]
Во внутреннем интеграле стоит математическое ожидание нормального закона с мат.ожиданием
$ry$ и дисперсией $(1-r^2)$. Поэтому внетренний интеграл равен $ry$. Теперь осталось
вычислить интеграл
\[
r \int_{-\infty}^\infty y^2\dfrac{e^{-y^2/2}}{\sqrt{2\pi}}\,dy=r,
\]
так как интеграл есть дисперсия стандартного нормального закона с параметрами $0$ и
$1$. Окончательно имеем: 
\[
\cov(X,Y)=r\sigma_1\sigma_2.
\]

Физический смысл ковариации: выберем направление на плоскости, задавшись направляющими
косинусами $(\theta_1, \theta_2)$, $\theta^2_1+\theta_2^2=1$. Тогда проекция на единичный
вектор $(\theta_1, \theta_2)$ отклонения $(X-\M X, Y-\M Y)$ случайной точки $(X,Y)$ от ее
среднего положения $(\M X, \M Y)$ равна (нарисовать рисунок)
\[
\theta_1(X-\M X)+ \theta_2( Y-\M Y).
\]
Естественно измерять степень разброса случайной точки $(X, Y)$ относительно ее среднего
положения в выбранном направлении с помощью дисперсии
\begin{multline*}
\var(\theta_1(X-\M X)+ \theta_2( Y-\M Y)) = \theta_1^2 \var X +
2\theta_1\theta_2\cov(X,Y)+ \theta_2^2 \var Y =
 \\ =
 (\theta_1, \theta_2) 
 \begin{pmatrix}
   \var X & \cov(X,Y)\\
   \cov(X,Y) & \var Y
 \end{pmatrix}
 \begin{pmatrix}\theta_1\\ \theta_2
 \end{pmatrix} \geqslant 0 \qquad \text{(потому что дисперсия)}
 .
\end{multline*}
Таким образом, ковариация позволяет находить разброс случайного вектора в заданном
направлении. 

\begin{definition}
  Пусть случайные величины  $X_1$, $X_2$, \ldots $X_n$ имеют конечные математические
  ожидания. Обозначим $\sigma_i^2=\var X_i$, $\sigma_{i,j}=\cov(X)i, X_j)$, $i\neq
  j$. Матрица 
  \[
  \Sigma=
  \begin{pmatrix}
    \sigma_1^2 & \sigma_{1,2} & \ldots & \sigma_{1,n}\\
    \sigma_{2,1} & \sigma_{2}^2 & \ldots & \sigma_{2,n}\\
    \vdots & \vdots & \ddots & \vdots\\
    \sigma_{n,1} & \sigma_{n,2} & \ldots & \sigma_n^2
  \end{pmatrix}
  \]
  называется \emph{ковариационной матрицей} этих случайных величин.
\end{definition}

\underline{Свойства ковариаций:}
\begin{enumerate}
\item Симметричность: $\cov(X,Y)=\cov(Y,X)$ (следует из определения)
\item Линейность: $\cov(a_1 X_1+a_2 X_2, Y) = a_1\cov(X_1, Y) + a_2\cov(X_2, Y)$
  (элементарно, доказать самим)
\item Вычислительна формула: $\cov(X,Y) = \M (X \cdot Y) - (\M X)\cdot(\M Y)$ (несложно,
  доказать самим).
\item Для независимых случайных величин $\cov(X,Y)=0$. Действительно, из свойства
  мат.ожидания, для независимых величин $\M(XY)=\M X \M Y$. В этой связи,
  \begin{definition}
    Случайные величины называются \emph{некоррелированными}, если $\cov(X,Y)\hm=0$. 
  \end{definition}
  Итак, независимые случайные величины некоррелированы. Обратное утверждение не
  верно. \par
  \underline{Пример.} Пусть случайные величины $X$ и $Y$ имеют равномерное распределение в
  эллипсе $D=\bigl\{(u,v)\colon \frac{u^2}{9}+\frac{v^2}{4}\leqslant1\bigr\}$. Их совместная
  плотность равна 
  \[
  p(u,v)=
  \begin{cases}
    \dfrac{1}{6\pi}&, (u,v)\in D,\\
    0,  & (u,v)\not\in D.
  \end{cases}
  \]
  Предлагается проверить дома самостоятельно, что частные плотности равны
  $p_X(u)\hm=\dfrac{2\sqrt{9-x^2}}{9\pi}$ при $|u|\leqslant3$,
  $p_Y(v)\hm=\dfrac{\sqrt{4-v^2}}{2\pi}$ при $|y|\leqslant2$. Из соображений симметрии, оба
  мат.ожидания равны нулю. Также из-за симметричности области $D$, 
  \[
  \cov(X,Y)=\dfrac{1}{6\pi}\iint\limits_D uv\,dydv=0.
  \]
  То есть, имеет место некоррелированность. Но также легко видеть, что $p(u,v)\neq p_X(u)
  p_Y(v)$ и случайные величины зависимы в вероятностном смысле.
\item Неравенство Чебышёва: $|\cov(X,Y)|\leqslant \sqrt{\var X \var Y}$. Равенство имеет
  место в случае линейной зависимости: $X+t_1Y=t_2$. \par
  \emph{Доказательство.} Для произвольного дейтсвительного $t$ имеем:
  \[
  \var(X+tY)=\var X + 2t\cov(X,Y)+t^2 \var Y \geqslant0.
  \]
  Неравенство имеет место для всех $t$, когда дискриминант квадратного трехчлена
  неположителен:
  \[
  4\cov(X,Y)^2-4(\var X)(\var Y)\leqslant0.
  \]
  Это неравенство эквивалентно доказываемому. Если дискриминант равен нулю, то 
  дисперсия $D(X+t_1Y)=0$ для $t_1=\sqrt{\var X \var Y}$, откуда с вероятностью единица
  (по свойствам мат. ожидания) $X+ t_1 Y = t_2$. 
\hfill Q.E.D.
\end{enumerate}

\begin{definition}
  Величина 
  \[
  \corr(X,Y)=\dfrac{\cov(X,Y)}{\sqrt{\var X \var Y}}
  \]
  называется \emph{коэффициентом корреляции} между величинами $X$ и $Y$.
\end{definition}

\underline{Свойства коэффициента корреляции:}
\begin{enumerate}
\item Симметричность: $\corr(X,Y)=\corr(Y,X)$ (следует из определения)
\item Для независимых случайных величин $\corr(X,Y)=0$.
\item Равенство $|\corr(X,Y)|=1$ имеет место тогда и только тогда, когда величины $X$ и
  $Y$ линейно связаны.
\end{enumerate}



Если останется время:
\underline{Одно применение ковариации}.\par
Пусть мы хотим научиться прогнозировать значение величины $Y$, наблюдая величину
$X$. Критерий эффективности --- средний квадрат погрешности. Если для прогноза
используется функция $y=f(x)$, то хотим $M(Y-f(X))^2\to\min$. Ограничимся линейными
прогнозами:
$f(x)=ax+b$. Рассмотрим функцию 
$$
L(a,b)=\M(Y-aX-b)^2 = \M Y^2+a^2\M X^2 + b^2
-2a\M(XY)-2b\M Y + 2ab \M X.$$
Надо решить задачу на минимум функции от двух переменных. Необходимое условие экстремума:
\begin{align*}
  \dfrac{\partial L}{\partial a} & = 2a\M X^2-2\M(XY)+2b\M X=0,\\
  \dfrac{\partial L}{\partial b} & = 2b-2\M Y+2a\M X=0.
\end{align*}
Решение имеет вид
\[
a=\dfrac{\M(XY)-\M X \M Y}{\M X^2-(\M X)^2} = \dfrac{\cov(X,Y)}{\var X}, \quad
b = \M Y - \M X \cdot \dfrac{\cov(X,Y)}{\var X}.
\]
Значит, прогноз имеет вид
\[
f(X)=\M Y+ \dfrac{\cov(X,Y)}{\var X}(X-\M X).
\]
Анализ результата: 1) если случайные величины некоррелированы, то прогноз не зависит от
$X$ и равен среднему значению $Y$; 2) если ковариация положительна, то значения $X>\M X$
советуют прибавлять к среднему значению $Y$ известную положительную поправку и наоборот,
при $X<\M X$ надо вычитать положительную поправку из среднего значения $Y$. Другими
словами, при положительной корреляции бОльшие значения $X$ <<намекают>> на бОльшие
значения $y$. Конечно, это не обязательно будет в одном опыте, но в среднем так в большом
числе опытов. При отрицательной ковариации увеличение $X$ приводит к уменьшению прогноза $Y$.
\end{document}
